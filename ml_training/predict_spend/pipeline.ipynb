{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17d27db-a86c-407d-8717-b94d00712cbb",
   "metadata": {},
   "source": [
    "## Run a Custom Training Job in Vertex AI (No Containerized Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f04c471-ffad-4a9e-b9b5-cf62e24ac34e",
   "metadata": {},
   "source": [
    "## 1 - Pip Install Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3565c1b9-a5bb-43ec-a7ec-88435be989b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --user --force-reinstall 'google-cloud-aiplatform>=1.15' -q --no-warn-conflicts\n",
    "! pip3 install google-cloud-pipeline-components -q --no-warn-conflicts\n",
    "! pip3 install google-cloud-bigquery -q --no-warn-conflicts\n",
    "! pip3 install {USER_FLAG} tensorflow==2.15.0 --upgrade -q --no-warn-conflicts\n",
    "! pip3 install {USER_FLAG} pandas\n",
    "! pip3 install {USER_FLAG} db-dtypes\n",
    "! pip3 install {USER_FLAG} scikit-learn\n",
    "! pip3 install {USER_FLAG} kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c4bc1d-d357-4ce6-849c-bb23cca25132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1996d92-89e5-44f5-a566-65408089f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doublecheck version of tensorflow \n",
    "! pip3 freeze | grep tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96b0cf3-bba7-42b6-bec2-37ae8ec73c18",
   "metadata": {},
   "source": [
    "## 2 - Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c8a746-5572-44a7-969b-6b1dc1635b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set project ID\n",
    "\n",
    "import os\n",
    "\n",
    "project_id = \"\" #set to your project ID\n",
    "location = '' #set to your region, for example us-central1\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    project_id = shell_output[0]\n",
    "    print(\"Project ID: \", project_id)\n",
    "\n",
    "\n",
    "pipeline_bucket_name = f'${project_id}-mlops-spend'\n",
    "pipeline_root_path = f'gs://{pipeline_bucket_name}'\n",
    "service_account = \"pipeline-sa@${project_id}.iam.gserviceaccount.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf656d4-6a6b-4c25-a10d-92ef40c20530",
   "metadata": {},
   "source": [
    "## 3 - Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d79648a9-816d-4316-8c41-4089ea3de7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components.v1.dataset import TabularDatasetCreateOp\n",
    "from google_cloud_pipeline_components.v1.automl.training_job import AutoMLTabularTrainingJobRunOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "from google_cloud_pipeline_components.v1.model import ModelExportOp\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "from google.cloud import storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d5199-4279-485d-81d9-19f665ed39de",
   "metadata": {},
   "source": [
    "## 4 - Define the workflow of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bc52b1b-05a2-4306-ac93-531e8a42e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"automl-tabular-training-v2\",\n",
    "    pipeline_root=pipeline_root_path)\n",
    "def pipeline(project_id: str):\n",
    "    ds_op = TabularDatasetCreateOp(\n",
    "        project=project_id,\n",
    "        location = location,\n",
    "        display_name=\"spend-dataset-pipelines\",\n",
    "        bq_source=f\"bq://{project_id}.unified_data.spend\"\n",
    "    )\n",
    "\n",
    "    training_job_run_op = AutoMLTabularTrainingJobRunOp(\n",
    "        dataset=ds_op.outputs[\"dataset\"],\n",
    "        target_column=\"spend_virtual_currency_value\",\n",
    "        project=project_id,\n",
    "        display_name=\"spend-automl-pipelines-minimize-rmse\",\n",
    "        model_display_name=\"ispend-automl-pipelines-minimize-rmse\",\n",
    "        optimization_prediction_type=\"regression\",\n",
    "        budget_milli_node_hours=1000,\n",
    "        optimization_objective=\"minimize-rmse\"\n",
    "    )\n",
    "\n",
    "    create_endpoint_op = EndpointCreateOp(\n",
    "        project=project_id,\n",
    "        display_name = \"spend-automl-pipelines-minimize-rmse-endpoint\",\n",
    "    )\n",
    "\n",
    "    model_deploy_op = ModelDeployOp(\n",
    "        model=training_job_run_op.outputs[\"model\"],\n",
    "        endpoint=create_endpoint_op.outputs['endpoint'],\n",
    "        dedicated_resources_machine_type = \"n1-highmem-4\",\n",
    "        dedicated_resources_accelerator_type = \"ACCELERATOR_TYPE_UNSPECIFIED\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    )\n",
    "\n",
    "    model_export_op = ModelExportOp(\n",
    "        model=training_job_run_op.outputs[\"model\"],\n",
    "        export_format_id=\"tf-saved-model\",\n",
    "        artifact_destination=pipeline_root_path\n",
    "    )\n",
    "    \n",
    "    model_export_op.set_caching_options(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8765bc90-0ea1-488f-a47d-9decdfd93f8b",
   "metadata": {},
   "source": [
    "## 5 - Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97cec28b-157d-4162-9e98-9b32626c8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the pipeline\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path='spend_pipeline.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c15a1d",
   "metadata": {},
   "source": [
    "## 6 - Upload the compiled file to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f382e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the compiled YAML file to Google Cloud Storage\n",
    "storage_client = storage.Client(project=project_id)\n",
    "bucket = storage_client.bucket(pipeline_bucket_name)\n",
    "blob = bucket.blob('spend_pipeline.yaml')\n",
    "\n",
    "# Upload the file \n",
    "blob.upload_from_filename('spend_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88d7b59-b549-4178-ad79-a5fe8f200fbb",
   "metadata": {},
   "source": [
    "## 6 - Prepare the pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a7fc2-949c-40c3-8c13-80b57003c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Vertex AI\n",
    "aiplatform.init(\n",
    "    project=project_id,\n",
    "    location=location,\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"spend-pipeline\",\n",
    "    template_path=\"spend_pipeline.yaml\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    parameter_values={\n",
    "        'project_id': project_id\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871be15b-7bd7-420d-8c5e-011511f8c4aa",
   "metadata": {},
   "source": [
    "## 7 - Submit the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e8eb4-2c14-40aa-80b7-8225d30c9478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the job\n",
    "job.submit(\n",
    "    service_account = service_account\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
